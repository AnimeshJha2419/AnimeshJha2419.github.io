{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bba6abd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91801\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91801\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download the necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba0091",
   "metadata": {},
   "source": [
    "os - This is a standard library in Python that provides a way to interact with the operating system. It is used in this code to access the file system and navigate through directories.\n",
    "\n",
    "glob - This is another standard library in Python that provides a way to retrieve files that match a certain pattern. It is used in this code to retrieve all the files in a directory that have a certain file extension.\n",
    "\n",
    "nltk - This is the Natural Language Toolkit, a popular library in Python for working with human language data. It provides a set of tools for tasks such as tokenization, stemming, and stopword removal.\n",
    "\n",
    "stopwords - This is a corpus of common stopwords that can be used to filter out words that are not useful for analysis. The stopwords module from nltk is used to download and access this corpus.\n",
    "\n",
    "word_tokenize - This is a function from nltk that splits a sentence into individual words. It is used in this code to tokenize the text data in the input files.\n",
    "\n",
    "SnowballStemmer - This is a stemming algorithm from nltk that reduces words to their base or root form. It is used in this code to normalize words and reduce the dimensionality of the data.\n",
    "\n",
    "TfidfVectorizer - This is a class from the sklearn library that creates a vector representation of text data based on the term frequency-inverse document frequency (tf-idf) weighting scheme. It is used in this code to create a vector representation of each input file.\n",
    "\n",
    "cosine_similarity - This is a function from the sklearn library that calculates the cosine similarity between two vectors. It is used in this code to compare the similarity between each pair of input files based on their vector representations.\n",
    "\n",
    "Overall, these libraries are used to preprocess the input files and convert them into a numerical representation that can be compared to detect plagiarism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa3d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocesses a given text by tokenizing it into words,\n",
    "    removing stopwords and punctuation, and stemming the words.\n",
    "    \"\"\"\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('arabic'))\n",
    "    words = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "    # Stem the words\n",
    "    stemmer = SnowballStemmer('arabic')\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Join the words back into a string\n",
    "    text = \" \".join(words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7369bbd6",
   "metadata": {},
   "source": [
    "Function called preprocess that pre-processes a given text in preparation for plagiarism detection.\n",
    "\n",
    "The function starts by tokenizing the text into words using the word_tokenize function from the nltk.tokenize module. The resulting tokens list contains all the words in the text.\n",
    "\n",
    "Next, the function removes stopwords and punctuation from the tokens list. Stopwords are common words in a language that are generally not useful for text analysis (such as \"the\", \"a\", and \"an\" in English). Punctuation marks are also removed since they are not useful for detecting plagiarism.\n",
    "\n",
    "After removing stopwords and punctuation, the function stems the words using the Snowball stemming algorithm from the nltk.stem module. Stemming reduces words to their base or root form, which helps to reduce the number of unique words in the text and improve the efficiency of the plagiarism detection algorithm.\n",
    "\n",
    "Finally, the function joins the stemmed words back into a string and returns it. This pre-processed text will be used later in the plagiarism detection algorithm to calculate the similarity between texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24956087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(folder):\n",
    "    \"\"\"\n",
    "    Trains the plagiarism checker by reading the contents of all files\n",
    "    in the given folder, preprocessing them, and vectorizing them using\n",
    "    the TF-IDF method.\n",
    "    \"\"\"\n",
    "    # Read the contents of all files in the folder\n",
    "    files = glob.glob(os.path.join(folder, \"*.txt\"))\n",
    "    texts = []\n",
    "    for file in files:\n",
    "        with open(file, encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            texts.append(text)\n",
    "    \n",
    "    # Preprocess the texts\n",
    "    preprocessed_texts = [preprocess(text) for text in texts]\n",
    "    \n",
    "    # Create a TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Vectorize the preprocessed texts\n",
    "    vectors = vectorizer.fit_transform(preprocessed_texts)\n",
    "    \n",
    "    return vectors, vectorizer, files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad75568",
   "metadata": {},
   "source": [
    "The train function is responsible for training the plagiarism checker by reading the contents of all files in the given folder, preprocessing them, and vectorizing them using the TF-IDF (Term Frequency-Inverse Document Frequency) method.\n",
    "\n",
    "First, it uses the glob module to find all files in the folder with a .txt extension. Then, it reads the contents of each file and stores them in a list called texts.\n",
    "\n",
    "Next, it preprocesses the texts by calling the preprocess function on each text in texts. This involves tokenizing each text into words, removing stop words and punctuation, and stemming the remaining words.\n",
    "\n",
    "After preprocessing, it creates a TfidfVectorizer object, which will be used to convert the preprocessed texts into vectors using the TF-IDF method. The fit_transform method of the vectorizer is then called on the preprocessed texts, which returns a matrix of TF-IDF values for each word in each text.\n",
    "\n",
    "Finally, the function returns the TF-IDF vectors, the vectorizer object, and a list of the file paths for each text in the folder.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b4c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(file, vectors, vectorizer, files):\n",
    "    \"\"\"\n",
    "    Compares the similarity between the given file and all other files\n",
    "    in the trained dataset using cosine similarity.\n",
    "    \"\"\"\n",
    "    # Read the contents of the file\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Preprocess the text\n",
    "    preprocessed_text = preprocess(text)\n",
    "    \n",
    "    # Vectorize the preprocessed text\n",
    "    query_vector = vectorizer.transform([preprocessed_text])\n",
    "    \n",
    "    # Calculate the cosine similarity between the query vector and the vectors of all texts in the folder\n",
    "    similarities = cosine_similarity(query_vector, vectors)[0]\n",
    "    \n",
    "    # Find the index of the text with the highest similarity\n",
    "    index = similarities.argmax()\n",
    "    \n",
    "    # Calculate the percentage similarity rounded to two decimal points\n",
    "    percentage_similarity = round(similarities[index] * 100, 2)\n",
    "    \n",
    "    # Get the filename of the text with the highest similarity\n",
    "    filename = os.path.basename(files[index])\n",
    "    \n",
    "    return filename, percentage_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb6b85",
   "metadata": {},
   "source": [
    "This function compares the similarity between a given file and all other files in the trained dataset using cosine similarity. Here's how it works:\n",
    "\n",
    "It reads the contents of the file.\n",
    "It preprocesses the text by using the preprocess() function to tokenize the text into words, remove stopwords and punctuation, and stem the words.\n",
    "It vectorizes the preprocessed text using the vectorizer object created during training.\n",
    "It calculates the cosine similarity between the query vector and the vectors of all texts in the folder using the cosine_similarity() function from scikit-learn.\n",
    "It finds the index of the text with the highest similarity.\n",
    "It calculates the percentage similarity rounded to two decimal points.\n",
    "It gets the filename of the text with the highest similarity.\n",
    "It returns the filename and percentage similarity.\n",
    "Overall, this function uses cosine similarity to compare the similarity between the given file and all other files in the trained dataset, and returns the filename and percentage similarity of the most similar file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00748bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar file: text7.txt\n",
      "Percentage similarity: 61.56%\n"
     ]
    }
   ],
   "source": [
    "# Define the folder containing the training data and the file to compare\n",
    "folder = r\"C:\\Users\\91801\\Desktop\\Trainingdata\"\n",
    "file = r\"C:\\Users\\91801\\Documents\\sample text.txt\"\n",
    "\n",
    "# Train the plagiarism checker on the training data\n",
    "vectors, vectorizer, files = train(folder)\n",
    "\n",
    "# Compare the given file with the trained dataset and print the results\n",
    "filename, percentage_similarity = compare(file, vectors, vectorizer, files)\n",
    "print(f\"Most similar file: {filename}\")\n",
    "print(f\"Percentage similarity: {percentage_similarity}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec8483",
   "metadata": {},
   "source": [
    "This code defines a folder containing the training data and a file to compare for plagiarism. It then trains the plagiarism checker on the training data using the train() function defined earlier, which reads the contents of all files in the folder, preprocesses them, and vectorizes them using the TF-IDF method. The compare() function is then called with the file to compare and the trained data. This function preprocesses the given file, vectorizes it using the TF-IDF method, calculates the cosine similarity between the vector of the given file and the vectors of all texts in the folder, finds the index of the text with the highest similarity, calculates the percentage similarity, and returns the filename of the text with the highest similarity and the percentage similarity.\n",
    "\n",
    "Finally, the filename and percentage similarity are printed for the user to see. This allows the user to determine if the given file is plagiarized and which file it is most similar to in the training dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
